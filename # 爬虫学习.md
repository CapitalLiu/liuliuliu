# 爬虫学习

## 前期准备

在cmd中使用**pip install requests** 命令安装requests库。
在IDLE 中测试requests是否正常。

为了后期能有更好的python开发环境，在某大佬的指导下同时也下载了pycharm和anaconda进行环境搭建。其中anaconda的环境搭建与我而言较为复杂。

安装完毕后![](C:\Users\Lenovo-1\Pictures\Screenshots\下载anaconda.png)

还要进行环境变量path的构建，以及使用清华源镜像帮助我们优化环境

但是在cmd中使用conda create方法创造库的时候遭到了异常报错

![](C:\Users\Lenovo-1\Pictures\Screenshots\QQ图片20220916115717.png)

发现问题有两个，一个是VPN没有关闭，在使用HTTPS时遭到了拦截，另一个是镜像源的问题。最后 **关闭了VPN，清除了channels，重新使用正确的镜像源，终于成功创建了自己的库。**

## requests库的学习


### HTTP协议中有六大方法：

requests.get():获取URL指向网页的全部内容

requests.head():获取网页头信息，相当于一个纲要，当信息较多时使用。

requests.post():在URL储存位置后添加新的信息。

requests.put():请求储存一个资源，并覆盖原来的资源。

requests.patch()：请求局部更新URL位置的资源。

requests.delete():顾名思义，删除储存位置的资源。

requests.request(method , url , \*\*kwsrgs)
method:对应七种方法。URL：对应的网络链接。\*\*kwargs：可选，有十三个参数。

### requests中的十三个参数
* params:一个字典或者字节序列，能作为参数增加到url中。
* files: 字典类型 ，文件名为值。可传输文件。
* timeout:设置超时时间，以秒为单位。
* json: JSON格式的数据，作为request中提交的内容‘
* data：向服务器中提交信息时使用。可以是字典，字符，或者文件。
* header： 对应request中的head字段，用于自己定制head。
* cookies：作为request中的cookie。
* auth：元组，支持HTTP认证功能。
* proxies：字典类型，设定访问的代理服务器，用于隐藏爬虫的源地址。
* allow_redirect:默认Ture，重定向开关。
* stream：默认为true，获取内容立即下载的开关。
* verity：默认为TRUE，认证SSL证书的开关。
* cert：本地SSL的开关。

### request中常见的异常
* requests.ConnnectionError                               网络连接异常 
* requests.HTTPError                                            HTTP错误异常
* requests.TooManyRedirects                              超过最大重定向次数
* requests.ConnectTimeut                                   连接远程服务器超时
* requests.timeout                                                 请求URL超时
* r.raise_for_status()                                               如果不是200，产生requests.HTTPError报错


### 爬取网页的通用代码框架
```python
import requests

def getHTMLText(url):
    try:
        r = requests.get(url, timeout=20)
        r.raise_for_status()
        r.encoding = r.apparent_encoding#使文本可以正常解析,通常是默认ISO。
    expect：
    	return "产生异常"
    
if __name__ =="__main__"
	url = "http://www.baidu.com"
    print(getHTMLText（url）)

```

**在request后会得到一个response对象 ，包含返回的所有内容。
`` r = requests.get(url)``
此时我们将response对象实例化换成 r
`` 使用 print（r.stasus_code）``
查询是否访问成功
`` 使用 r.header r.text ``
获得request的头文件信息以及全部的文本资源。



### 爬虫初级实例

1.爬取京东商品信息。

![](C:\Users\Lenovo-1\Pictures\Screenshots\京东商品爬虫实践.png)

2.亚马逊爬取实例

由于亚马逊网站的限制，不允许爬虫类型进行访问。所以我们在header上进行了修改。

![](C:\Users\Lenovo-1\Pictures\Screenshots\爬取亚马逊.png)

3.百度或360浏览器的关键词爬取

以百度为例：http：//www.baidu.com/s?wd=keyword

此时要用字典的形式在s后方加上关键词参数。

![](C:\Users\Lenovo-1\Pictures\Screenshots\百度搜索daima.png)



4.爬取图片或其他格式的内容 及储存。

以图片（jpg）为例，在链接的后方要加上jpg的格式。同时还得附上本机保存的位置（path），以二进制（r.content）的形式保存进去。这时也会使用到os库，以及文件的打开读写和关闭的方法。

![](C:\Users\Lenovo-1\Pictures\Screenshots\爬取图片.png)

5. 查询一个URL的IP地址

使用到第三方网站进行查询，将要查询的网站的IP地址一起提交进去。

![](C:\Users\Lenovo-1\Pictures\Screenshots\IP地址查询.png)

## beautiful soup的使用

**Beautiful Soup库是解析，遍历，维护“标签树”（HTML）的功能库。
HTML的结构：



![](C:\Users\Lenovo-1\Pictures\Camera Roll\HTML基本格式.png)

使用soup.a ,soup.p获取a标签及p标签的内容。

**以及tag里面的基本元素

![](C:\Users\Lenovo-1\Pictures\Camera Roll\美汤里的基本元素.png)

如：用soup.a.name 获取a标签的名字。

### 使用BeautifulSoup的实例：

![](C:\Users\Lenovo-1\Pictures\Camera Roll\美汤的使用方法.png)

### 美汤里面的遍历方法
1.下行遍历：

![](C:\Users\Lenovo-1\Pictures\Camera Roll\下行遍历.png)

其中children 和 descendants方法返回的是一个可迭代对象，只能用for….in语句阅读

2.上行遍历

![](C:\Users\Lenovo-1\Pictures\Camera Roll\上行遍历.png)

parents返回的是迭代对象。

3.平行遍历

![](C:\Users\Lenovo-1\Pictures\Camera Roll\平行遍历.png)

### 美汤中的HTML输出

bs4 库的prettify（）方法能在标签后加上换行符，使得标签内容输出后更方便阅读

## 信息的标记和提取

### 信息的标记

HTML（超文本标记语言）通过标签的形式将超文本的内容插入到文本当中去。

信息标记的三种方式：

* XML:  HTML的拓展，与其类似，通过<name>attribute</name>的方式表示标签。

  ![](C:\Users\Lenovo-1\Pictures\Camera Roll\xml.png)

* JSON：全称为JavasScript Object Notation 由有类型（字符串，数字等）的键值对构成。可嵌套。

  ![](C:\Users\Lenovo-1\Pictures\Camera Roll\json.png)

* YAML：由无类型的键值对组成，由缩进形式表达所属关系，'—' 表达并列，'|'表达整块的信息。'#'  表示注释。

  ![](C:\Users\Lenovo-1\Pictures\Camera Roll\YAML.png)

### 信息的提取

1.通过解析信息类型后，使用该类型专门的解析器进行分析。
2.直接搜索关键词，忽略文本的格式。

 最好的方法是将两种方法相结合。
 先将文本正确解读，再用find__all()方法进行查询。

![](C:\Users\Lenovo-1\Pictures\Camera Roll\解析格式法.png)

find__all的 用法及其参数：

![](C:\Users\Lenovo-1\Pictures\Camera Roll\find all.png)

find的其他方法：

![](C:\Users\Lenovo-1\Pictures\Camera Roll\find拓展方法.png)

**<tag>.find__all()的缩写方法为<tag>（）**

## 爬虫的实践1：中国大学排名

**网页信息**

![](C:\Users\Lenovo-1\Pictures\Camera Roll\中国大学排名原版.png)





**源代码**

![](C:\Users\Lenovo-1\Pictures\Camera Roll\中国大学排名源代码.png)

**成果：**

![](C:\Users\Lenovo-1\Pictures\Camera Roll\中国大学排名.png)

## Re库的学习

在re库中使用  r‘…..’表示原生的字符串（raw string），即不包含转义字符的字符串。

re库的方法：

![](C:\Users\Lenovo-1\Pictures\Saved Pictures\re库方法.png)

![](C:\Users\Lenovo-1\Pictures\Saved Pictures\参数.png)

![](C:\Users\Lenovo-1\Pictures\Saved Pictures\flags.png)

**需要强调的是search是从字符串中所有范围内开始匹配正则表达式，而match则是从开头的地方开始匹配。他们都返回的是match的对象。使用match.group(0)获取字符串。**

findall返回的是列表的形式。

split还有一个限定分割最大次数的参数，将匹配部分抽走，剩下内容进行分割。

sub还有一个最大替换次数的参数**

另一种使用方法：
先使用^^ pat = re.compile(r'[1-9]\d{5}')^^ 进行编译生成一个实例化的对象（这才是真的正则表达式）
然后用这个pat对象继续使用re库的六种方法。适用于大型函数多次调用的情况。

### match对象

![](C:\Users\Lenovo-1\Pictures\Saved Pictures\match属性.png)

![](C:\Users\Lenovo-1\Pictures\Saved Pictures\match对象的方法.png)
